{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachintha\\anaconda3\\envs\\ppi_dl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Bio.PDB import MMCIFParser\n",
    "from Bio.PDB.Polypeptide import is_aa\n",
    "from Bio.SeqUtils import seq1\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch \n",
    "import os \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachintha\\anaconda3\\envs\\ppi_dl\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(40000, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-29): 30 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inializing here\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False)\n",
    "model = BertModel.from_pretrained('Rostlab/prot_bert_bfd')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2protbert(seq):\n",
    "    # Tokenize the sequence\n",
    "    seq = ' '.join(seq)\n",
    "    inputs = tokenizer(seq, return_tensors='pt', add_special_tokens=True, padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get ProtBERT embeddings\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # ProtBERT embeddings to numpy\n",
    "    embeddings = embeddings #.detach().cpu() #.numpy()\n",
    "    attention_mask = attention_mask #.detach().cpu() #.numpy()\n",
    "    features = []\n",
    "    for seq_num in range(len(embeddings)):\n",
    "        seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "        if seq_len > 2:\n",
    "            seq_emd = embeddings[seq_num][1:seq_len-1]  # without [CLS] and [SEP]\n",
    "            features.append(seq_emd)\n",
    "\n",
    "    # Convert list of arrays to 2D array\n",
    "    if features:\n",
    "        features_2d = torch.vstack(features).to('cpu')  # Stack all sequences into a 2D array\n",
    "        return features_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to hold contact maps for each chain in edge index format\n",
    "contact_maps = {}\n",
    "init_node_features = [] # save seperate init AA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distance threshold for contacts\n",
    "distance_threshold = 10.0\n",
    "\n",
    "# convert adj map into edge index format for contact maps\n",
    "def contact_map_to_edge_index(contact_map):\n",
    "    # Step 1: Find the indices where there is a contact (i.e., where the value is 1)\n",
    "    row, col = torch.nonzero(contact_map, as_tuple=True)\n",
    "\n",
    "    # Step 2: Stack the row and col indices to create the edge index\n",
    "    edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create contact maps and get embeddings for AA per chain\n",
    "def extract_ContactMap_SeqEmbedds(file_name):\n",
    "    # file_name ='4hhb.cif'\n",
    "    pdb_id = file_name.split('\\\\')[-1].split('.')[0]\n",
    "    structure = MMCIFParser(QUIET=True).get_structure(pdb_id, file_name)\n",
    "    \n",
    "    # Iterate through chains\n",
    "    for model in structure:\n",
    "        model_id = model.get_id()\n",
    "        # print(model_id)\n",
    "        for chain in model:\n",
    "            chain_id = chain.get_id()\n",
    "            ca_atoms = []\n",
    "            chain_seq = ''\n",
    "            for residue in chain:\n",
    "                if is_aa(residue) and 'CA' in residue:\n",
    "                    chain_seq += seq1(residue.resname)\n",
    "                    ca_atoms.append(residue['CA'].get_coord())\n",
    "\n",
    "            num_atoms = len(ca_atoms)\n",
    "            print('ca atom count ', num_atoms)#\n",
    "\n",
    "            if len(chain_seq) > 60:\n",
    "                node_features = seq2protbert(chain_seq)\n",
    "                init_node_features.append(node_features)\n",
    "                # Initialize a contact map matrix\n",
    "                contact_map = torch.zeros((num_atoms, num_atoms))\n",
    "\n",
    "                    # Calculate distances between all pairs of C-alpha atoms\n",
    "                for i in range(num_atoms):  # through each c-alpha atom\n",
    "                    for j in range(i+1, num_atoms): # cal euclidean norm with other c-alphas = total c-alphas - 1\n",
    "                        distance = np.linalg.norm(ca_atoms[i] - ca_atoms[j])\n",
    "                        # distance = ca_atoms[i].get - ca_atoms[j]    # cal euclidean distance\n",
    "                        if distance <= 10.0 and distance > 0:\n",
    "                            contact_map[i, j] = 1\n",
    "                            contact_map[j, i] = 1\n",
    "                # print(contact_map)\n",
    "\n",
    "                    # Save the contact map in the dictionary\n",
    "                map_name = f'{pdb_id}_{model_id}_{chain_id}'\n",
    "                print(map_name)\n",
    "\n",
    "                    # Step 2: Stack the row and col indices to create the edge index\n",
    "                edge_index = contact_map_to_edge_index(contact_map)\n",
    "                # global contact_maps\n",
    "                contact_maps[map_name] = edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge seperate contact maps in edge index format into a single combined edge index\n",
    "def combine_contact_maps(contact_maps):\n",
    "    # Cumulative offset for node indices\n",
    "    cumulative_offset = 0\n",
    "    combined_edge_index = []\n",
    "\n",
    "    for edge_index in contact_maps:\n",
    "        # Adjust the node indices for each graph\n",
    "        adjusted_edge_index = edge_index + cumulative_offset\n",
    "        combined_edge_index.append(adjusted_edge_index)\n",
    "\n",
    "        # Update the cumulative offset based on the max node index in the current graph\n",
    "        cumulative_offset += edge_index.max().item() + 1\n",
    "\n",
    "    # Concatenate all edge indices into one big edge index\n",
    "    combined_edge_index = torch.cat(combined_edge_index, dim=1)\n",
    "    # print(combined_edge_index)\n",
    "    return combined_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_init_node_features(node_features_list):\n",
    "    init_node_features = torch.cat(node_features_list, dim=0)\n",
    "    return init_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path\n",
    "folder_path = 'D:\\\\year 4\\\\semester 1\\\\BT\\\\BT 4033\\\\structure\\\\temp\\\\'\n",
    "\n",
    "# List all files in the specified folder\n",
    "file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    file_path = folder_path + file\n",
    "    try:\n",
    "        extract_ContactMap_SeqEmbedds(file_name=file_path)\n",
    "        print(file, ' done ************************************\\n')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# print(init_node_features)\n",
    "final_AA_list = combine_contact_maps(contact_maps.values())\n",
    "node_features = return_init_node_features(init_node_features)\n",
    "print(final_AA_list)\n",
    "print(node_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_AA_list, 'aa_edge_indices.pt')\n",
    "torch.save(node_features, 'init_node_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppi_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
